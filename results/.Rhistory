makeDiscreteParam("min.node.size", values = 1:10),
makeDiscreteParam("mtry", values = 1:5)
)
# Budget
f.evals = 40
mbo.init.design.size = 30
# Focus search
infill.opt = "focussearch"
mbo.focussearch.points = 100
mbo.focussearch.maxit = 3
mbo.focussearch.restarts = 3
# The MLR Learner
classif.lrn = makeLearner("classif.ranger", predict.type = "response")
library(mlrMBO)
# The final SMOOF objective function
objFun = makeMultiObjectiveFunction(
name = "reg",
fn = performan,
par.set = ps,
has.simple.signature = FALSE,
noisy = TRUE,
n.objectives = 1,
minimize = c(TRUE)
)
# Build the control object
method = "parego"
if (method == "parego") {
mbo.prop.points = 10
mbo.crit = "cb"
parego.crit.cb.pi = 0.5
}
control = makeMBOControl(n.objectives = 1L, propose.points = mbo.prop.points)
control = setMBOControlTermination(control, max.evals =  f.evals, iters = 300)
control = setMBOControlInfill(control, crit = mbo.crit, opt = infill.opt,
opt.focussearch.maxit = mbo.focussearch.maxit,
opt.focussearch.points = mbo.focussearch.points,
opt.restarts = mbo.focussearch.restarts,
crit.cb.pi = parego.crit.cb.pi, crit.cb.lambda = NULL)
mbo.learner = makeLearner("regr.randomForest", predict.type = "se")
design = generateDesign(mbo.init.design.size, smoof::getParamSet(objFun), fun = lhs::maximinLHS)
set.seed(123)
result = mbo(fun = objFun, design = design, learner = mbo.learner, control = control)
result
x = list(mtry = 10, min.node.size = 1, sample.fraction = 0.6, replace = FALSE)
pred <- ranger(formula = target ~ . , data = train,  mtry = x$mtry, min.node.size = x$min.node.size,
)
system.time(pred <- ranger(formula = target ~ . , data = train,  mtry = x$mtry, min.node.size = x$min.node.size,
sample.fraction = x$sample.fraction, replace = x$replace, num.trees = 1000)$predictions)
pred
train$target
y = train$target
y
y * log(pred)
log(pred)
y[1:2]
log(pred[1:2])
y * log(pred)
c(y * log(pred))[1:2]
log(pred)[1:2]
library(ranger)
library(mlrMBO)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
performan = function(x) {
pred <- ranger(formula = target ~ . , data = train,  mtry = x$mtry, min.node.size = x$min.node.size,
sample.fraction = x$sample.fraction, replace = x$replace, num.trees = 1000)$predictions
y = train$target
return(-mean(y * log(pred) + (1 - y) * log(1 - pred)))
}
parallelMap::parallelStartMulticore(level = "mlrMBO.feval")
?parallelMap::parallelStartMulticore
library(ranger)
library(mlrMBO)
?makeDiscreteParam
# Its ParamSet
ps = makeParamSet(
makeLogicalParam("replace"),
makeIntegerParam("min.node.size", lower = 1, upper = 1000),
makeNumericParam("sample.fraction", lower = 0.2, upper = 1),
makeIntegerParam("mtry", lower = 1, upper = 100)
)
# Its ParamSet
ps = makeParamSet(
makeLogicalParam("replace"),
makeIntegerParam("min.node.size", lower = 1, upper = 1000),
makeNumericParam("sample.fraction", lower = 0.2, upper = 1),
makeIntegerParam("mtry", lower = 1, upper = 50)
)
# Budget
f.evals = 40
mbo.init.design.size = 30
# Focus search
infill.opt = "focussearch"
mbo.focussearch.points = 100
mbo.focussearch.maxit = 3
mbo.focussearch.restarts = 3
library(mlrMBO)
# The final SMOOF objective function
objFun = makeMultiObjectiveFunction(
name = "reg",
fn = performan,
par.set = ps,
has.simple.signature = FALSE,
noisy = TRUE,
n.objectives = 1,
minimize = c(TRUE)
)
# Build the control object
method = "parego"
if (method == "parego") {
mbo.prop.points = 1
mbo.crit = "cb"
parego.crit.cb.pi = 0.5
}
control = makeMBOControl(n.objectives = 1L, propose.points = mbo.prop.points)
control = setMBOControlTermination(control, max.evals =  f.evals, iters = 300)
control = setMBOControlInfill(control, crit = mbo.crit, opt = infill.opt,
opt.focussearch.maxit = mbo.focussearch.maxit,
opt.focussearch.points = mbo.focussearch.points,
opt.restarts = mbo.focussearch.restarts,
crit.cb.pi = parego.crit.cb.pi, crit.cb.lambda = NULL)
mbo.learner = makeLearner("regr.randomForest", predict.type = "se")
design = generateDesign(mbo.init.design.size, smoof::getParamSet(objFun), fun = lhs::maximinLHS)
performan = function(x) {
pred <- ranger(formula = target ~ . , data = train,  mtry = x$mtry, min.node.size = x$min.node.size,
sample.fraction = x$sample.fraction, replace = x$replace, num.trees = 1000)$predictions
y = train$target
return(-mean(y * log(pred) + (1 - y) * log(1 - pred)))
}
# Budget
f.evals = 40
mbo.init.design.size = 30
# Focus search
infill.opt = "focussearch"
mbo.focussearch.points = 100
mbo.focussearch.maxit = 3
mbo.focussearch.restarts = 3
library(mlrMBO)
# The final SMOOF objective function
objFun = makeMultiObjectiveFunction(
name = "reg",
fn = performan,
par.set = ps,
has.simple.signature = FALSE,
noisy = TRUE,
n.objectives = 1,
minimize = c(TRUE)
)
# Build the control object
method = "parego"
if (method == "parego") {
mbo.prop.points = 1
mbo.crit = "cb"
parego.crit.cb.pi = 0.5
}
control = makeMBOControl(n.objectives = 1L, propose.points = mbo.prop.points)
control = setMBOControlTermination(control, max.evals =  f.evals, iters = 300)
control = setMBOControlInfill(control, crit = mbo.crit, opt = infill.opt,
opt.focussearch.maxit = mbo.focussearch.maxit,
opt.focussearch.points = mbo.focussearch.points,
opt.restarts = mbo.focussearch.restarts,
crit.cb.pi = parego.crit.cb.pi, crit.cb.lambda = NULL)
mbo.learner = makeLearner("regr.randomForest", predict.type = "se")
design = generateDesign(mbo.init.design.size, smoof::getParamSet(objFun), fun = lhs::maximinLHS)
set.seed(123)
result = mbo(fun = objFun, design = design, learner = mbo.learner, control = control)
library(ranger)
library(mlrMBO)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
performan = function(x) {
pred <- ranger(formula = target ~ . , data = train,  mtry = x$mtry, min.node.size = x$min.node.size,
sample.fraction = x$sample.fraction, replace = x$replace, num.trees = 1000)$predictions
y = train$target
return(-mean(y * log(pred) + (1 - y) * log(1 - pred)))
}
# Its ParamSet
ps = makeParamSet(
makeLogicalParam("replace"),
makeIntegerParam("min.node.size", lower = 1, upper = 1000),
makeNumericParam("sample.fraction", lower = 0.2, upper = 1),
makeIntegerParam("mtry", lower = 1, upper = 50)
)
# Budget
f.evals = 40
mbo.init.design.size = 30
# Focus search
infill.opt = "focussearch"
mbo.focussearch.points = 100
mbo.focussearch.maxit = 3
mbo.focussearch.restarts = 3
library(mlrMBO)
# The final SMOOF objective function
objFun = makeMultiObjectiveFunction(
name = "reg",
fn = performan,
par.set = ps,
has.simple.signature = FALSE,
noisy = TRUE,
n.objectives = 1,
minimize = c(TRUE)
)
# Build the control object
method = "parego"
if (method == "parego") {
mbo.prop.points = 1
mbo.crit = "cb"
parego.crit.cb.pi = 0.5
}
control = makeMBOControl(n.objectives = 1L, propose.points = mbo.prop.points)
control = setMBOControlTermination(control, max.evals =  f.evals, iters = 300)
control = setMBOControlInfill(control, crit = mbo.crit, opt = infill.opt,
opt.focussearch.maxit = mbo.focussearch.maxit,
opt.focussearch.points = mbo.focussearch.points,
opt.restarts = mbo.focussearch.restarts,
crit.cb.pi = parego.crit.cb.pi, crit.cb.lambda = NULL)
mbo.learner = makeLearner("regr.randomForest", predict.type = "se")
design = generateDesign(mbo.init.design.size, smoof::getParamSet(objFun), fun = lhs::maximinLHS)
set.seed(123)
result = mbo(fun = objFun, design = design, learner = mbo.learner, control = control)
result
pred <- ranger(formula = target ~ . , data = train,  mtry = x$mtry, min.node.size = x$min.node.size,
sample.fraction = x$sample.fraction, replace = x$replace, num.trees = 1000)$predictions
x = list(mtry = 1, min.node.size = 1, sample.fraction = 1, replace = TRUE)
pred <- ranger(formula = target ~ . , data = train,  mtry = x$mtry, min.node.size = x$min.node.size,
sample.fraction = x$sample.fraction, replace = x$replace, num.trees = 1000)$predictions
y = train$target
-mean(y * log(pred) + (1 - y) * log(1 - pred))
library(xgboost)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
xgtrain = xgb.DMatrix(as.matrix(train[, -1]), label = train$target)
xgtest = xgb.DMatrix(as.matrix(test))
# set parameters of xgboost
param <- list(
# some generic, non specific params
"objective"  = "binary:logistic"
, "eval_metric" = "logloss"
, "eta" = 0.04
, "subsample" = 0.9
, "colsample_bytree" = 0.9
, "min_child_weight" = 1
, "max_depth" = 10
)
xgboost_ensemble <- rep(0, nrow(test))
for (i in 1:20) {
print(i)
model = xgb.train(
nrounds = 500
, params = param
, data = xgtrain
, print.every.n = 20
, nthread = 8
)
pred <- predict(model, xgtest)
xgboost_ensemble = xgboost_ensemble + pred
}
library(xgboost)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
xgtrain = xgb.DMatrix(as.matrix(train[, -1]), label = train$target)
xgtest = xgb.DMatrix(as.matrix(test))
# set parameters of xgboost
param <- list(
# some generic, non specific params
"objective"  = "binary:logistic"
, "eval_metric" = "logloss"
, "eta" = 0.04
, "subsample" = 0.9
, "colsample_bytree" = 0.9
, "min_child_weight" = 1
, "max_depth" = 10
)
xgboost_ensemble <- rep(0, nrow(test))
for (i in 1:30) {
print(i)
model = xgb.train(
nrounds = 500
, params = param
, data = xgtrain
, print.every.n = 20
, nthread = 8
)
pred <- predict(model, xgtest)
xgboost_ensemble = xgboost_ensemble + pred
}
library(xgboost)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
xgtrain = xgb.DMatrix(as.matrix(train[, -1]), label = train$target)
xgtest = xgb.DMatrix(as.matrix(test))
# set parameters of xgboost
param <- list(
# some generic, non specific params
"objective"  = "binary:logistic"
, "eval_metric" = "logloss"
, "eta" = 0.04
, "subsample" = 0.9
, "colsample_bytree" = 0.9
, "min_child_weight" = 1
, "max_depth" = 10
)
xgboost_ensemble <- rep(0, nrow(test))
set.seed(1)
for (i in 1:30) {
print(i)
model = xgb.train(
nrounds = 500
, params = param
, data = xgtrain
, print.every.n = 20
, nthread = 8
)
pred <- predict(model, xgtest)
xgboost_ensemble = xgboost_ensemble + pred
}
xgboost_ensemble = xgboost_ensemble/i
save(xgboost_ensemble, file = "/home/probst/Kaggle/BNPParibas/results/xgboost.RData")
test  <- read_csv("/home/probst/Kaggle/BNPParibas/data/test.csv")
library(readr)
test  <- read_csv("/home/probst/Kaggle/BNPParibas/data/test.csv")
submission <- data.frame(ID=test$ID, PredictedProb=xgboost_ensemble)
write_csv(submission, "/home/probst/Kaggle/BNPParibas/results/xgboost_30_restr_feat.csv")
load("/home/probst/Kaggle/BNPParibas/results/xgboost.RData")
load("/home/probst/Kaggle/BNPParibas/results/extratrees_100.RData")
?makeStackedLearner
library(mlr)
a_1 = 0.95
a_2 = 0.05
pred = a_1 * xgboost_ensemble + a_2 * extratrees_100
library(readr)
test  <- read_csv("/home/probst/Kaggle/BNPParibas/data/test.csv")
pred
submission <- data.frame(ID=test$ID, PredictedProb=pred)
write_csv(submission, "/home/probst/Kaggle/BNPParibas/results/ensemble_xgboost_extratrees.csv")
library(deepboost)
install.packages("deepboost")
library(deepboost)
?deepboost
library(rpart)
?rpart
library(mlr)
library(glmnet)
lrn = makeLearner("regr.glmnet")
getParamSet(lrn)
library(mlr)
library(glmnet)
lrn = makeLearner("regr.glmnet")
getParamSet(lrn)
?"glmnet"
lrn$par.set
lrn$par.set[]
lrn$par.set$pars$family
lrn$par.set$pars$family[]
library(mlr)
library(glmnet)
lrn = makeLearner("classif.glmnet")
getParamSet(lrn)
model = ranger(formula = target ~ . , data = train,  mtry = 34, min.node.size = 293, sample.fraction = 0.8796, replace = FALSE, num.trees = 3000)
library(ranger)
library(mlrMBO)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
model = ranger(formula = target ~ . , data = train,  mtry = 34, min.node.size = 293, sample.fraction = 0.8796, replace = FALSE, num.trees = 3000)
test  <- read_csv("/home/probst/Kaggle/BNPParibas/data/test.csv")
library(readr)
test  <- read_csv("/home/probst/Kaggle/BNPParibas/data/test.csv")
submission <- data.frame(ID=test$ID, PredictedProb=ranger_3000)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
ranger_3000 = predict(model, test)
model = ranger(formula = target ~ . , data = train,  mtry = 34, min.node.size = 293, sample.fraction = 0.8796, #
replace = FALSE, num.trees = 3000, write.forest = TRUE)
ranger_3000 = predict(model, test)
save(ranger_3000, file = "/home/probst/Kaggle/BNPParibas/results/ranger_3000.RData")
submission <- data.frame(ID=test$ID, PredictedProb=ranger_3000)
write_csv(submission, "/home/probst/Kaggle/BNPParibas/results/ranger_3000.csv")
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
submission <- data.frame(ID=test$ID, PredictedProb=ranger_3000)
write_csv(submission, "/home/probst/Kaggle/BNPParibas/results/ranger_3000.csv")
ranger_3000
ranger_3000$predictions
ranger 3000 = ranger_3000$predictions
ranger_3000 = ranger_3000$predictions
save(ranger_3000, file = "/home/probst/Kaggle/BNPParibas/results/ranger_3000.RData")
submission <- data.frame(ID=test$ID, PredictedProb=ranger_3000)
ranger_3000
length(ranger_3000)
length(test)
length(test$ID)
test  = read_csv("/home/probst/Kaggle/BNPParibas/data/test.csv")
submission <- data.frame(ID=test$ID, PredictedProb=ranger_3000)
write_csv(submission, "/home/probst/Kaggle/BNPParibas/results/ranger_3000.csv")
library(mlr)
library(readr)
load("/home/probst/Kaggle/BNPParibas/results/xgboost.RData")
load("/home/probst/Kaggle/BNPParibas/results/extratrees_100.RData")
load("/home/probst/Kaggle/BNPParibas/results/ranger_3000.RData")
test  <- read_csv("/home/probst/Kaggle/BNPParibas/data/test.csv")
pred = a_1 * xgboost_ensemble + a_2 * extratrees_100 + a_3 * ranger_3000
a_1 = 0.8
a_2 = 0.15
a_3 = 0.05
pred = a_1 * xgboost_ensemble + a_2 * extratrees_100 + a_3 * ranger_3000
submission <- data.frame(ID=test$ID, PredictedProb=pred)
write_csv(submission, "/home/probst/Kaggle/BNPParibas/results/ensemble_xgboost_extratrees_100_ranger_3000.csv")
library(ranger)
library(mlrMBO)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
model_ranger_3000 = ranger(formula = target ~ . , data = train,  mtry = 34, min.node.size = 293, sample.fraction = 0.8796, #
}}
set.seed(123)
model_ranger_3000 = ranger(formula = target ~ . , data = train,  mtry = 34, min.node.size = 293, sample.fraction = 0.8796, #
replace = FALSE, num.trees = 3000, write.forest = TRUE)
ranger_3000 = predict(model_ranger_3000, test)$predictions
save(model_ranger_3000, ranger_3000, file = "/home/probst/Kaggle/BNPParibas/results/ranger_3000.RData")
python = read_csv("/home/probst/Kaggle/BNPParibas/results/python.csv")
options( java.parameters = "-Xmx2g" )
library(extraTrees)
load("/home/probst/Kaggle/BNPParibas/data/data.RData")
set.seed(1)
model_extratrees_100 = extraTrees(train[, -1], train$target, ntree = 100, numRandomCuts = 2, numThreads = 5)
extratrees_100 = predict(model, test)
set.seed(1)
model_extratrees_150 = extraTrees(train[, -1], train$target, ntree = 150, numRandomCuts = 2, numThreads = 5)
extratrees_150 = predict(model, test)
extratrees_150[extratrees_150 >= 1] = 0.999999999
extratrees_150[extratrees_150 <= 0] = 0.000000001
save(model_extratrees_150, extratrees_150, file = "/home/probst/Kaggle/BNPParibas/results/extratrees_150.RData")
extratrees_150 = predict(model_extratrees_150, test)
extratrees_150_train = predict(model_extratrees_150, train)
head(train)
extratrees_150_train = predict(model_extratrees_150, train[, -1])
extratrees_150[extratrees_150 >= 1] = 0.999999999
extratrees_150[extratrees_150 <= 0] = 0.000000001
extratrees_150_train[extratrees_150_train >= 1] = 0.999999999
extratrees_150_train[extratrees_150_train <= 0] = 0.000000001
save(extratrees_150_train, extratrees_150, file = "/home/probst/Kaggle/BNPParibas/results/extratrees_150.RData")
library(randomForest)
install.packages("randomForest")
?randomForest
library(randomForest)
?randomForest
options(java.parameters = "- Xmx1024m") # Should avoid java gc overhead
library(OpenML)
install.packages("OpenML")
install.packages("devtools")
install.packages("ranger")
install.packages("mlr")
install.packages(c("mlr", "checkmate", "data.table", "digest", "RCurl", "stringi", "XML", "RWeka", "devtools"))
devtools::install_github("openml/r")
install.packages("party")
install.packages("BatchExperiments")
library(BatchExperiments)
install.packages("BBmisc")
library(BBmisc)
library(BatchExperiments)
library(BatchJobs)
install.packages("BatchJobs")
install.packages("stringi")
library(stringi)
options(java.parameters = "- Xmx1024m") # Should avoid java gc overhead
library(OpenML)
install.packages("plyr")
library(OpenML)
options(java.parameters = "- Xmx1024m") # Should avoid java gc overhead
library(OpenML)
install.packages("scales")
library(randomForestSRC)
library(OpenML)
options(java.parameters = "- Xmx1024m") # Should avoid java gc overhead
library(OpenML)
install.packages("reshape2")
options(java.parameters = "- Xmx1024m") # Should avoid java gc overhead
library(OpenML)
install.packages("htmltools")
library(OpenML)
install.packages("httpuv")
options(java.parameters = "- Xmx1024m") # Should avoid java gc overhead
library(OpenML)
install.packages("dplyr")
options(java.parameters = "- Xmx1024m") # Should avoid java gc overhead
library(OpenML)
library(BatchExperiments)
library(mlr)
dir = "/nfsmb/koll/probst/Random_Forest/RFSplitbias"
setwd(paste0(dir,"/results"))
load(paste0(dir,"/results/clas.RData"))
load(paste0(dir,"/results/reg.RData"))
tasks = reg
?makeResampleDesc
load(paste0(dir,"/results/clas.RData"))
load(paste0(dir,"/results/reg.RData"))
head(reg)
reg[reg$NumberOfNumericFeatures == 0]
reg[reg$NumberOfNumericFeatures == 1]
reg[reg$NumberOfNumericFeatures == 1,]
reg[reg$NumberOfNumericFeatures == 0,]
reg[reg$NumberOfNumericFeatures == 2,]
reg[reg$NumberOfNumericFeatures == 3,]
reg[reg$NumberOfNumericFeatures == 1,]
clas[clas$NumberOfNumericFeatures == 1,]
clas[clas$NumberOfNumericFeatures == 0,]
task = getOMLTask(task.id = 3700, verbosity=0)$input$data.set
head(task)
reg[reg$NumberOfNumericFeatures == 1,]
task = getOMLTask(task.id = 4820, verbosity=0)$input$data.set
head(task)
nrow(clas[clas$NumberOfNumericFeatures == 0,])
nrow(clas_small[clas_small$NumberOfNumericFeatures == 0,])
nrow(reg_small[reg_small$NumberOfNumericFeatures == 1,])
clas_small$NumberOfNumericFeatures
clas_small$NumberOfFeatures
clas_small$NumberOfSymbolicFeatures
clas_small$NumberOfNumericFeatures + clas_small$NumberOfSymbolicFeatures + 1 == clas_small$NumberOfFeatures
clas_small$NumberOfNumericFeatures + clas_small$NumberOfSymbolicFeatures + 1 - clas_small$NumberOfFeatures
head(clas_small)
library(party)
?cforest
